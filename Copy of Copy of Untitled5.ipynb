{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1UmiICx2CHQ4rJMoAJ0X2UgtEeHNlalz-","timestamp":1710017375315}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"ed5d82b2051d452d98645e23e5e2efab":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_dc3cd51fe6114acc896c188703bea113","IPY_MODEL_9a4438f8de0b43b68566ad8fd380dc70","IPY_MODEL_5e6aae0d1ec74277b9c34ea82346b51f"],"layout":"IPY_MODEL_bc5aa76e9a0a43538adc0321ed654752"}},"dc3cd51fe6114acc896c188703bea113":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d829d0d26df84730bb3e7930d239bb11","placeholder":"​","style":"IPY_MODEL_e1fb41174dc641129e2f3ce926239c0f","value":"tokenizer_config.json: 100%"}},"9a4438f8de0b43b68566ad8fd380dc70":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8231fc6357e648a1b9bd1f5005b76a41","max":88,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0277bdc89101471eb5be2068a34da3d3","value":88}},"5e6aae0d1ec74277b9c34ea82346b51f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a6a08ef219fc493185b9da89c98b28e1","placeholder":"​","style":"IPY_MODEL_b88a95ad6dae44e5a800de357f81a73f","value":" 88.0/88.0 [00:00&lt;00:00, 5.98kB/s]"}},"bc5aa76e9a0a43538adc0321ed654752":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d829d0d26df84730bb3e7930d239bb11":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e1fb41174dc641129e2f3ce926239c0f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8231fc6357e648a1b9bd1f5005b76a41":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0277bdc89101471eb5be2068a34da3d3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a6a08ef219fc493185b9da89c98b28e1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b88a95ad6dae44e5a800de357f81a73f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7e78efe27af84459af9fbe1649e961f4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e35e4ef62766457093eb831face465a9","IPY_MODEL_864fa9aef9834cb890c2ef776f4ae9de","IPY_MODEL_db743ebaf6bf4478816ae833f426d3f2"],"layout":"IPY_MODEL_49b96856de0a40c6a5d45a7019cff731"}},"e35e4ef62766457093eb831face465a9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ebaad42abe614f0a8b9877b760ec8005","placeholder":"​","style":"IPY_MODEL_95f5a0db41f84bb59d1f4a30441a24df","value":"config.json: 100%"}},"864fa9aef9834cb890c2ef776f4ae9de":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_60b1f347eea54930bd240a027d9ba4e3","max":1120,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d5ddf14f487b442d9b51195ad7d3a3a3","value":1120}},"db743ebaf6bf4478816ae833f426d3f2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f779fa35760242649db590922158cebf","placeholder":"​","style":"IPY_MODEL_b49c0342a14c429da7b4a823dd8cd509","value":" 1.12k/1.12k [00:00&lt;00:00, 65.9kB/s]"}},"49b96856de0a40c6a5d45a7019cff731":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ebaad42abe614f0a8b9877b760ec8005":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"95f5a0db41f84bb59d1f4a30441a24df":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"60b1f347eea54930bd240a027d9ba4e3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d5ddf14f487b442d9b51195ad7d3a3a3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f779fa35760242649db590922158cebf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b49c0342a14c429da7b4a823dd8cd509":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0aa98410afa74064bf1a0a46b7271abf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_448fc1c178c046ff8b02da57d68fbdf0","IPY_MODEL_8b1645f0efd34af8bfb952b98c3f75f0","IPY_MODEL_bcdf545652e242f7959eae0d847592e5"],"layout":"IPY_MODEL_1f3eb30ccc8745faad65ed100a414403"}},"448fc1c178c046ff8b02da57d68fbdf0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c9087c4602a54d188fdc7141ca83fcaa","placeholder":"​","style":"IPY_MODEL_10c10b278801482a8dc7c9a4e2a3d188","value":"spiece.model: 100%"}},"8b1645f0efd34af8bfb952b98c3f75f0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f806c338def94dec9aeec1adff8372f3","max":1912529,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9ea08f77e308471bb802b01c2d48eb71","value":1912529}},"bcdf545652e242f7959eae0d847592e5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a6140867cdc441278fc817f9a60ff232","placeholder":"​","style":"IPY_MODEL_d72d2541ae684a45a95d8a3ec3246689","value":" 1.91M/1.91M [00:00&lt;00:00, 9.35MB/s]"}},"1f3eb30ccc8745faad65ed100a414403":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9087c4602a54d188fdc7141ca83fcaa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"10c10b278801482a8dc7c9a4e2a3d188":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f806c338def94dec9aeec1adff8372f3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9ea08f77e308471bb802b01c2d48eb71":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a6140867cdc441278fc817f9a60ff232":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d72d2541ae684a45a95d8a3ec3246689":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f2167fa566c54cf1bb8daedd89b585cd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_82740ca3b5104cb983b84f238741c116","IPY_MODEL_fa01310a6bf1434e8c6b32b0ce40fa46","IPY_MODEL_f8b6fffadf0b415fb1105e8f2b179856"],"layout":"IPY_MODEL_c7e5ede9a23a4f72ad7da7a1646088c4"}},"82740ca3b5104cb983b84f238741c116":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c1f5a23ca4ab4f58838d185334e59e3e","placeholder":"​","style":"IPY_MODEL_febd37e77040400da83e8c4477664435","value":"special_tokens_map.json: 100%"}},"fa01310a6bf1434e8c6b32b0ce40fa46":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_121e73e2caba43bcaaa21ff974765d61","max":65,"min":0,"orientation":"horizontal","style":"IPY_MODEL_47044e6063fc4eb2a06df6cff120b46a","value":65}},"f8b6fffadf0b415fb1105e8f2b179856":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c9bc2ed505d541f090f4b550b7b4be57","placeholder":"​","style":"IPY_MODEL_1d238a8e3bb1421fafb3200ed4bb5623","value":" 65.0/65.0 [00:00&lt;00:00, 4.51kB/s]"}},"c7e5ede9a23a4f72ad7da7a1646088c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c1f5a23ca4ab4f58838d185334e59e3e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"febd37e77040400da83e8c4477664435":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"121e73e2caba43bcaaa21ff974765d61":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"47044e6063fc4eb2a06df6cff120b46a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c9bc2ed505d541f090f4b550b7b4be57":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1d238a8e3bb1421fafb3200ed4bb5623":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"93da8c1efe534183bebcac3d5db5f8ac":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_27d526ca93f1412d849d528a198afbdf","IPY_MODEL_4b40277afbb64edc8f928445dec1917c","IPY_MODEL_86a16f0962fd45f49cf7103321237414"],"layout":"IPY_MODEL_75172aa542374d7aa34104634481d971"}},"27d526ca93f1412d849d528a198afbdf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b4aac2b5bf534757bdc5cc383730f726","placeholder":"​","style":"IPY_MODEL_e6a3cdd8894741d6a9cf1b3a9dfc3599","value":"pytorch_model.bin: 100%"}},"4b40277afbb64edc8f928445dec1917c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fffe9eab5ee04d7da92a24d7d11c1642","max":2275327883,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8dac8cc5bfb34773ae68c9bda8d3aaab","value":2275327883}},"86a16f0962fd45f49cf7103321237414":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7730cc29c7d343519904bd22d4360ee6","placeholder":"​","style":"IPY_MODEL_6dd18f71d6cf4299b6516bac8a81b7c3","value":" 2.28G/2.28G [00:15&lt;00:00, 168MB/s]"}},"75172aa542374d7aa34104634481d971":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4aac2b5bf534757bdc5cc383730f726":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e6a3cdd8894741d6a9cf1b3a9dfc3599":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fffe9eab5ee04d7da92a24d7d11c1642":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8dac8cc5bfb34773ae68c9bda8d3aaab":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7730cc29c7d343519904bd22d4360ee6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6dd18f71d6cf4299b6516bac8a81b7c3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b8c0c3a01a094eea8f763f68619a5653":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d56ec3e3dbf6483fa962b83d902b483f","IPY_MODEL_d2dbeb672ecd43f88e28a893ea6b0f35","IPY_MODEL_10e2b256f6b741ec874fc143417f0d25"],"layout":"IPY_MODEL_cb0434fd31c14d2caabf400d7b79165c"}},"d56ec3e3dbf6483fa962b83d902b483f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4ee4570085984aa8a4be448d316105ae","placeholder":"​","style":"IPY_MODEL_e9a7b83ab025480c8482d168653e543d","value":"generation_config.json: 100%"}},"d2dbeb672ecd43f88e28a893ea6b0f35":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7ec01714407a431eb8b7f5e373b2e212","max":280,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a4ec710afcd2441a92791c8bebf73b2d","value":280}},"10e2b256f6b741ec874fc143417f0d25":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d0472aa952494522b5d1ca85fc1d9db1","placeholder":"​","style":"IPY_MODEL_d91b7d469c0641c18c0973dc5cb77643","value":" 280/280 [00:00&lt;00:00, 10.9kB/s]"}},"cb0434fd31c14d2caabf400d7b79165c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ee4570085984aa8a4be448d316105ae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e9a7b83ab025480c8482d168653e543d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7ec01714407a431eb8b7f5e373b2e212":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a4ec710afcd2441a92791c8bebf73b2d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d0472aa952494522b5d1ca85fc1d9db1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d91b7d469c0641c18c0973dc5cb77643":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["!pip install transformers[sentencepiece] datasets sacrebleu rouge_score py7zr -q"],"metadata":{"id":"69EsO0FOdjoW","executionInfo":{"status":"ok","timestamp":1711039306147,"user_tz":-330,"elapsed":13280,"user":{"displayName":"Rishita Upadhyay","userId":"03411805649589442780"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9765acba-5bdf-4736-c665-3e947595e72c"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/510.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/510.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m501.8/510.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.6/106.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.3/412.3 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.9/138.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["! sudo apt install nvidia-utils-515"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eTAjiK8pAx-h","executionInfo":{"status":"ok","timestamp":1711039317639,"user_tz":-330,"elapsed":11498,"user":{"displayName":"Rishita Upadhyay","userId":"03411805649589442780"}},"outputId":"9ceb7875-9bca-4a8a-b432-5f3599d215af"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following additional packages will be installed:\n","  libnvidia-compute-515\n","Suggested packages:\n","  nvidia-driver-515\n","The following NEW packages will be installed:\n","  libnvidia-compute-515 nvidia-utils-515\n","0 upgraded, 2 newly installed, 0 to remove and 39 not upgraded.\n","Need to get 45.4 MB of archives.\n","After this operation, 194 MB of additional disk space will be used.\n","Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-compute-515 515.105.01-0ubuntu1 [45.0 MB]\n","Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-utils-515 515.105.01-0ubuntu1 [337 kB]\n","Fetched 45.4 MB in 1s (49.7 MB/s)\n","debconf: unable to initialize frontend: Dialog\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 2.)\n","debconf: falling back to frontend: Readline\n","debconf: unable to initialize frontend: Readline\n","debconf: (This frontend requires a controlling tty.)\n","debconf: falling back to frontend: Teletype\n","dpkg-preconfigure: unable to re-open stdin: \n","Selecting previously unselected package libnvidia-compute-515:amd64.\n","(Reading database ... 121753 files and directories currently installed.)\n","Preparing to unpack .../libnvidia-compute-515_515.105.01-0ubuntu1_amd64.deb ...\n","Unpacking libnvidia-compute-515:amd64 (515.105.01-0ubuntu1) ...\n","Selecting previously unselected package nvidia-utils-515.\n","Preparing to unpack .../nvidia-utils-515_515.105.01-0ubuntu1_amd64.deb ...\n","Unpacking nvidia-utils-515 (515.105.01-0ubuntu1) ...\n","Setting up libnvidia-compute-515:amd64 (515.105.01-0ubuntu1) ...\n","Setting up nvidia-utils-515 (515.105.01-0ubuntu1) ...\n","Processing triggers for man-db (2.10.2-1) ...\n","Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n"]}]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dCTPlLq2ekA8","executionInfo":{"status":"ok","timestamp":1711039318086,"user_tz":-330,"elapsed":6,"user":{"displayName":"Rishita Upadhyay","userId":"03411805649589442780"}},"outputId":"0dbd7165-0de1-4a44-8737-c4b8fd0ecce9"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Thu Mar 21 16:41:56 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   53C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["from transformers import pipeline, set_seed\n","\n","import matplotlib.pyplot as plt\n","from datasets import load_dataset\n","import pandas as pd\n","from datasets import load_dataset, load_metric\n","\n","from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n","\n","import nltk\n","from nltk.tokenize import sent_tokenize\n","\n","from tqdm import tqdm\n","import torch\n","\n","nltk.download(\"punkt\")\n"],"metadata":{"id":"oOTbDb9_e6qW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711039357347,"user_tz":-330,"elapsed":22625,"user":{"displayName":"Rishita Upadhyay","userId":"03411805649589442780"}},"outputId":"c5a54f0a-280d-45c4-8e6e-d2e06eb8aa79"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n","device"],"metadata":{"id":"QLVfNB-ugZg4","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1711039407455,"user_tz":-330,"elapsed":731,"user":{"displayName":"Rishita Upadhyay","userId":"03411805649589442780"}},"outputId":"f31e8888-6b94-48ac-f119-9db783d9bc17"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'cuda'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["model_ckpt = \"google/pegasus-cnn_dailymail\"\n"],"metadata":{"id":"u0VTKHkBgxfk","executionInfo":{"status":"ok","timestamp":1711039416996,"user_tz":-330,"elapsed":510,"user":{"displayName":"Rishita Upadhyay","userId":"03411805649589442780"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n","\n","model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)"],"metadata":{"id":"Qpj6LtEBhF4Q","colab":{"base_uri":"https://localhost:8080/","height":368,"referenced_widgets":["ed5d82b2051d452d98645e23e5e2efab","dc3cd51fe6114acc896c188703bea113","9a4438f8de0b43b68566ad8fd380dc70","5e6aae0d1ec74277b9c34ea82346b51f","bc5aa76e9a0a43538adc0321ed654752","d829d0d26df84730bb3e7930d239bb11","e1fb41174dc641129e2f3ce926239c0f","8231fc6357e648a1b9bd1f5005b76a41","0277bdc89101471eb5be2068a34da3d3","a6a08ef219fc493185b9da89c98b28e1","b88a95ad6dae44e5a800de357f81a73f","7e78efe27af84459af9fbe1649e961f4","e35e4ef62766457093eb831face465a9","864fa9aef9834cb890c2ef776f4ae9de","db743ebaf6bf4478816ae833f426d3f2","49b96856de0a40c6a5d45a7019cff731","ebaad42abe614f0a8b9877b760ec8005","95f5a0db41f84bb59d1f4a30441a24df","60b1f347eea54930bd240a027d9ba4e3","d5ddf14f487b442d9b51195ad7d3a3a3","f779fa35760242649db590922158cebf","b49c0342a14c429da7b4a823dd8cd509","0aa98410afa74064bf1a0a46b7271abf","448fc1c178c046ff8b02da57d68fbdf0","8b1645f0efd34af8bfb952b98c3f75f0","bcdf545652e242f7959eae0d847592e5","1f3eb30ccc8745faad65ed100a414403","c9087c4602a54d188fdc7141ca83fcaa","10c10b278801482a8dc7c9a4e2a3d188","f806c338def94dec9aeec1adff8372f3","9ea08f77e308471bb802b01c2d48eb71","a6140867cdc441278fc817f9a60ff232","d72d2541ae684a45a95d8a3ec3246689","f2167fa566c54cf1bb8daedd89b585cd","82740ca3b5104cb983b84f238741c116","fa01310a6bf1434e8c6b32b0ce40fa46","f8b6fffadf0b415fb1105e8f2b179856","c7e5ede9a23a4f72ad7da7a1646088c4","c1f5a23ca4ab4f58838d185334e59e3e","febd37e77040400da83e8c4477664435","121e73e2caba43bcaaa21ff974765d61","47044e6063fc4eb2a06df6cff120b46a","c9bc2ed505d541f090f4b550b7b4be57","1d238a8e3bb1421fafb3200ed4bb5623","93da8c1efe534183bebcac3d5db5f8ac","27d526ca93f1412d849d528a198afbdf","4b40277afbb64edc8f928445dec1917c","86a16f0962fd45f49cf7103321237414","75172aa542374d7aa34104634481d971","b4aac2b5bf534757bdc5cc383730f726","e6a3cdd8894741d6a9cf1b3a9dfc3599","fffe9eab5ee04d7da92a24d7d11c1642","8dac8cc5bfb34773ae68c9bda8d3aaab","7730cc29c7d343519904bd22d4360ee6","6dd18f71d6cf4299b6516bac8a81b7c3","b8c0c3a01a094eea8f763f68619a5653","d56ec3e3dbf6483fa962b83d902b483f","d2dbeb672ecd43f88e28a893ea6b0f35","10e2b256f6b741ec874fc143417f0d25","cb0434fd31c14d2caabf400d7b79165c","4ee4570085984aa8a4be448d316105ae","e9a7b83ab025480c8482d168653e543d","7ec01714407a431eb8b7f5e373b2e212","a4ec710afcd2441a92791c8bebf73b2d","d0472aa952494522b5d1ca85fc1d9db1","d91b7d469c0641c18c0973dc5cb77643"]},"executionInfo":{"status":"ok","timestamp":1711039842351,"user_tz":-330,"elapsed":31321,"user":{"displayName":"Rishita Upadhyay","userId":"03411805649589442780"}},"outputId":"78b96356-12ea-4335-fe77-409b70b3e728"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/88.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed5d82b2051d452d98645e23e5e2efab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/1.12k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e78efe27af84459af9fbe1649e961f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["spiece.model:   0%|          | 0.00/1.91M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0aa98410afa74064bf1a0a46b7271abf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2167fa566c54cf1bb8daedd89b585cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["pytorch_model.bin:   0%|          | 0.00/2.28G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93da8c1efe534183bebcac3d5db5f8ac"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"display_data","data":{"text/plain":["generation_config.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8c0c3a01a094eea8f763f68619a5653"}},"metadata":{}}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"cJFwBTtdh-Qw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711039950686,"user_tz":-330,"elapsed":30883,"user":{"displayName":"Rishita Upadhyay","userId":"03411805649589442780"}},"outputId":"55c7084e-daa2-41b5-f20a-5e5454dd8c34"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["dataset=pd.read_csv(\"/content/drive/MyDrive/scisumm.csv\")"],"metadata":{"id":"fs5ABAuohbXD","executionInfo":{"status":"ok","timestamp":1711040319884,"user_tz":-330,"elapsed":3035,"user":{"displayName":"Rishita Upadhyay","userId":"03411805649589442780"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["dataset"],"metadata":{"id":"UiIqn4TMjAMN","colab":{"base_uri":"https://localhost:8080/","height":423},"executionInfo":{"status":"ok","timestamp":1711040327832,"user_tz":-330,"elapsed":740,"user":{"displayName":"Rishita Upadhyay","userId":"03411805649589442780"}},"outputId":"28046634-9b2d-4edd-c86b-5c82f8f05a60"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                   text  \\\n","0     TnT - A Statistical Part-Of-Speech Tagger Trig...   \n","1     Mildly Non-Projective Dependency Structures Sy...   \n","2     Using Corpus Statistics And WordNet Relations ...   \n","3     Automatic Labeling Of Semantic Roles present a...   \n","4     Generative Models For Statistical Parsing With...   \n","...                                                 ...   \n","1004  Combining Lexical Syntactic And Semantic Featu...   \n","1005  Similarity of Semantic Relations are at least ...   \n","1006  Further Meta-Evaluation of Machine Translation...   \n","1007  Soft Syntactic Constraints for Hierarchical Ph...   \n","1008  A Unification Method For Disjunctive Feature D...   \n","\n","                                                summary  \n","0     TnT - A Statistical Part-Of-Speech Tagger\\nTri...  \n","1     Mildly Non-Projective Dependency Structures\\nS...  \n","2     Using Corpus Statistics And WordNet Relations ...  \n","3     Automatic Labeling Of Semantic Roles\\nWe prese...  \n","4     Generative Models For Statistical Parsing With...  \n","...                                                 ...  \n","1004  Combining Lexical Syntactic And Semantic Featu...  \n","1005  Similarity of Semantic Relations\\nThere are at...  \n","1006  Further Meta-Evaluation of Machine Translation...  \n","1007  Soft Syntactic Constraints for Hierarchical Ph...  \n","1008  A Unification Method For Disjunctive Feature D...  \n","\n","[1009 rows x 2 columns]"],"text/html":["\n","  <div id=\"df-9df25f6f-03c9-4723-bed1-ae855edc1fb9\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>summary</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>TnT - A Statistical Part-Of-Speech Tagger Trig...</td>\n","      <td>TnT - A Statistical Part-Of-Speech Tagger\\nTri...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Mildly Non-Projective Dependency Structures Sy...</td>\n","      <td>Mildly Non-Projective Dependency Structures\\nS...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Using Corpus Statistics And WordNet Relations ...</td>\n","      <td>Using Corpus Statistics And WordNet Relations ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Automatic Labeling Of Semantic Roles present a...</td>\n","      <td>Automatic Labeling Of Semantic Roles\\nWe prese...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Generative Models For Statistical Parsing With...</td>\n","      <td>Generative Models For Statistical Parsing With...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1004</th>\n","      <td>Combining Lexical Syntactic And Semantic Featu...</td>\n","      <td>Combining Lexical Syntactic And Semantic Featu...</td>\n","    </tr>\n","    <tr>\n","      <th>1005</th>\n","      <td>Similarity of Semantic Relations are at least ...</td>\n","      <td>Similarity of Semantic Relations\\nThere are at...</td>\n","    </tr>\n","    <tr>\n","      <th>1006</th>\n","      <td>Further Meta-Evaluation of Machine Translation...</td>\n","      <td>Further Meta-Evaluation of Machine Translation...</td>\n","    </tr>\n","    <tr>\n","      <th>1007</th>\n","      <td>Soft Syntactic Constraints for Hierarchical Ph...</td>\n","      <td>Soft Syntactic Constraints for Hierarchical Ph...</td>\n","    </tr>\n","    <tr>\n","      <th>1008</th>\n","      <td>A Unification Method For Disjunctive Feature D...</td>\n","      <td>A Unification Method For Disjunctive Feature D...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1009 rows × 2 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9df25f6f-03c9-4723-bed1-ae855edc1fb9')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-9df25f6f-03c9-4723-bed1-ae855edc1fb9 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-9df25f6f-03c9-4723-bed1-ae855edc1fb9');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-c4c5365a-9541-480f-a3fb-788480c1a2e4\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c4c5365a-9541-480f-a3fb-788480c1a2e4')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-c4c5365a-9541-480f-a3fb-788480c1a2e4 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_bfb55faf-ee86-43fc-9219-d4cb5ed967d6\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('dataset')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_bfb55faf-ee86-43fc-9219-d4cb5ed967d6 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('dataset');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"dataset","summary":"{\n  \"name\": \"dataset\",\n  \"rows\": 1009,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1009,\n        \"samples\": [\n          \"Chinese Segmentation And New Word Detection Using Conditional Random Fields Chinese word segmentation is a difficult, im portant and widely-studied sequence modelingproblem. This paper demonstrates the abil ity of linear-chain conditional random fields(CRFs) to perform robust and accurate Chinese word segmentation by providing a principled framework that easily supports the in tegration of domain knowledge in the form of multiple lexicons of characters and words. We also present a probabilistic new word detection method, which further improves performance. Our system is evaluated on four datasets usedin a recent comprehensive Chinese word segmentation competition. State-of-the-art perfor mance is obtained. Unlike English and other western languages, many Asian languages such as Chinese, Japanese, and Thai, do not delimit words by white-space. Wordsegmentation is therefore a key precursor for language processing tasks in these languages. For Chinese, there has been significant research on find ing word boundaries in unsegmented sequences(see (Sproat and Shih, 2002) for a review). Un fortunately, building a Chinese word segmentation system is complicated by the fact that there is no standard definition of word boundaries in Chinese. Approaches to Chinese segmentation fall roughly into two categories: heuristic dictionary-based methods and statistical machine learning methods.In dictionary-based methods, a predefined dictio nary is used along with hand-generated rules for segmenting input sequence (Wu, 1999). Howeverthese approaches have been limited by the impossibility of creating a lexicon that includes all possible Chinese words and by the lack of robust statistical inference in the rules. Machine learning approaches are more desirable and have been successful in both unsupervised learning (Peng and Schuur mans, 2001) and supervised learning (Teahan et al, 2000). Many current approaches suffer from either lackof exact inference over sequences or difficulty in incorporating domain knowledge effectively into seg mentation. Domain knowledge is either not used, used in a limited way, or used in a complicated way spread across different components. For example,the N-gram generative language modeling based ap proach of Teahan et al(2000) does not use domainknowledge. Gao et al(2003) uses class-based language for word segmentation where some word cat egory information can be incorporated. Zhang et al (2003) use a hierarchical hidden Markov Model to incorporate lexical knowledge. A recent advance in this area is Xue (2003), in which the author uses a sliding-window maximum entropy classifier to tag Chinese characters into one of four position tags, and then covert these tags into a segmentation using rules. Maximum entropy models give tremendousflexibility to incorporate arbitrary features. How ever, a traditional maximum entropy tagger, as used in Xue (2003), labels characters without consideringdependencies among the predicted segmentation labels that is inherent in the state transitions of finite state sequence models. Linear-chain conditional random fields (CRFs) (Lafferty et al, 2001) are models that address both issues above. Unlike heuristic methods, they are principled probabilistic finite state models onwhich exact inference over sequences can be ef ficiently performed. Unlike generative N-gram or hidden Markov models, they have the ability to straightforwardly combine rich domain knowledge, for example in this paper, in the form of multiple readily-available lexicons. Furthermore, they arediscriminatively-trained, and are often more accurate than generative models, even with the same fea tures. In their most general form, CRFs are arbitrary undirected graphical models trained to maximize the conditional probability of the desired outputs given the corresponding inputs. In the linear-chainspecial case we use here, they can be roughly un derstood as discriminatively-trained hidden Markovmodels with next-state transition functions represented by exponential models (as in maximum en tropy classifiers), and with great flexibility to viewthe observation sequence in terms of arbitrary, over lapping features, with long-range dependencies, and at multiple levels of granularity. These beneficialproperties suggests that CRFs are a promising ap proach for Chinese word segmentation.New word detection is one of the most impor tant problems in Chinese information processing.Many machine learning approaches have been pro posed (Chen and Bai, 1998; Wu and Jiang, 2000; Nie et al, 1995). New word detection is normally considered as a separate process from segmentation.However, integrating them would benefit both seg mentation and new word detection. CRFs provide aconvenient framework for doing this. They can pro duce not only a segmentation, but also confidence in local segmentation decisions, which can be usedto find new, unfamiliar character sequences sur rounded by high-confidence segmentations. Thus, our new word detection is not a stand-alone process, but an integral part of segmentation. Newly detected words are re-incorporated into our word lexicon,and used to improve segmentation. Improved seg mentation can then be further used to improve new word detection. Comparing Chinese word segmentation accuracyacross systems can be difficult because many re search papers use different data sets and different ground-rules. Some published results claim 98% or99% segmentation precision and recall, but these ei ther count only the words that occur in the lexicon, or use unrealistically simple data, lexicons that haveextremely small (or artificially non-existant) outof-vocabulary rates, short sentences or many numbers. A recent Chinese word segmentation competition (Sproat and Emerson, 2003) has made compar isons easier. The competition provided four datasets with significantly different segmentation guidelines, and consistent train-test splits. The performance ofparticipating system varies significantly across different datasets. Our system achieves top performance in two of the runs, and a state-of-the-art per formance on average. This indicates that CRFs are a viable model for robust Chinese word segmentation. Conditional random fields (CRFs) are undirected graphical models trained to maximize a conditional probability (Lafferty et al, 2001). A common special-case graph structure is a linear chain, which corresponds to a finite state machine, and is suitablefor sequence labeling. A linear-chain CRF with parameters ? = {?1, ...} defines a conditional proba bility for a state (label) sequence y = y1...yT (for example, labels indicating where words start or have their interior) given an input sequence x = x1...xT (for example, the characters of a Chinese sentence) to be P?(y|x) = 1Zx exp ( T? t=1 ? k ?kfk(yt?1, yt,x, t) ) , (1) where Zx is the per-input normalization that makes the probability of all state sequences sum to one;fk(yt?1, yt,x, t) is a feature function which is of ten binary-valued, but can be real-valued, and ?k is a learned weight associated with feature fk. The feature functions can measure any aspect of a statetransition, yt?1 ? yt, and the entire observation se quence, x, centered at the current time step, t. For example, one feature function might have value 1when yt?1 is the state START, yt is the state NOT START, and xt is a word appearing in a lexicon of people?s first names. Large positive values for ?kindicate a preference for such an event; large nega tive values make the event unlikely. The most probable label sequence for an input x, y? = argmaxy P?(y|x),can be efficiently determined using the Viterbi algorithm (Rabiner, 1990). An N -best list of labeling sequences can also be obtained using modi fied Viterbi algorithm and A* search (Schwartz and Chow, 1990). The parameters can be estimated by maximum likelihood?maximizing the conditional probabilityof a set of label sequences, each given their cor responding input sequences. The log-likelihood of training set {(xi, yi) : i = 1, ...M} is written L? = ? i logP?(yi|xi) = ? i ( T? t=1 ? k ?kfk(yt?1, yt,x, t)? logZxi ) . Traditional maximum entropy learning algorithms, such as GIS and IIS (della Pietra et al, 1995), canbe used to train CRFs. However, our implemen tation uses a quasi-Newton gradient-climber BFGS for optimization, which has been shown to converge much faster (Malouf, 2002; Sha and Pereira, 2003). The gradient of the likelihood is ?P?(y|x)/??k = ? i,t fk(yt?1, y(i)t ,x(i), t) ? ? i,y,t P?(y|x(i))fk(yt?1, yt,x(i), t) CRFs share many of the advantageous properties of standard maximum entropy classifiers, including their convex likelihood function, which guarantees that the learning procedure converges to the global maximum. 2.1 Regularization in CRFs. To avoid over-fitting, log-likelihood is usually penalized by some prior distribution over the parameters. A commonly used prior is a zero-mean Gaussian. With a Gaussian prior, log-likelihood is penal ized as follows. L? = ? i logP?(yi|xi)? k ?2k 2?2k (2) where ?2k is the variance for feature dimension k. The variance can be feature dependent. However for simplicity, constant variance is often used for all features. We experiment an alternate version ofGaussian prior in which the variance is feature dependent. We bin features by frequency in the train ing set, and let the features in the same bin share the same variance. The discounted value is set to be ?k dck/Me??2 where ck is the count of features, M is the bin size set by held out validation, and dae is the ceiling function. See Peng and McCallum (2004) for more details and further experiments. 2.2 State transition features. Varying state-transition structures with different Markov order can be specified by different CRF feature functions, as determined by the number ofoutput labels y examined together in a feature func tion. We define four different state transition feature functions corresponding to different Markov orders.Higher-order features capture more long-range de pendencies, but also cause more data sparseness problems and require more memory for training. The best Markov order for a particular application can be selected by held-out cross-validation. 1. First-order: Here the inputs are examined in. the context of the current state only. The feature functions are represented as f(yt,x).There are no separate parameters for state tran sitions. 2. First-order+transitions: Here we add parame-. ters corresponding to state transitions. The fea ture functions used are f(yt,x), f(yt?1, yt).context of the current and previous states. Fea ture function are represented as f(yt?1, yt,x). 4. Third-order: Here inputs are examined in. the context of the current, and two previous states. Feature function are represented as f(yt?2, yt?1, yt,x). We cast the segmentation problem as one of se quence tagging: Chinese characters that begin a new word are given the START tag, and characters in the middle and at the end of words are given theNONSTART tag. The task of segmenting new, un segmented test data becomes a matter of assigning a sequence of tags (labels) to the input sequence of Chinese characters. Conditional random fields are configured as a linear-chain (finite state machine) for this purpose,and tagging is performed using the Viterbi algorithm to efficiently find the most likely label se quence for a given character sequence. 3.1 Lexicon features as domain knowledge. One advantage of CRFs (as well as traditional maximum entropy models) is its flexibility in using arbitrary features of the input. To explore this advantage, as well as the importance of domain knowledge, we use many open features from external re sources. To specifically evaluate the importance ofdomain knowledge beyond the training data, we divide our features into two categories: closed fea tures and open features, (i.e., features allowed in thecompetition?s ?closed test? and ?open test? respec tively). The open features include a large word list (containing single and multiple-character words), a character list, and additional topic or part-of-speech character lexicons obtained from various sources. The closed features are obtained from training data alone, by intersecting the character list obtainedfrom training data with corresponding open lexi cons. Many lexicons of Chinese words and characters are available from the Internet and other sources. Besides the word list and character list, our lexiconsinclude 24 lists of Chinese words and characters obtained from several Internet sites1 cleaned and augmented by a local native Chinese speaker indepen dently of the competition data. The list of lexicons used in our experiments is shown in Figure 1. 3.2 Feature conjunctions. Since CRFs are log-linear models, feature conjunctions are required to form complex, non-linear de cision boundaries in the original feature space. We 1http://www.mandarintools.com, ftp://xcin.linux.org.tw/pub/xcin/libtabe, http://www.geocities.com/hao510/wordlist noun (e.g.,?,?) verb (e.g.,?) adjective (e.g.,?,?) adverb (e.g.,!,?) auxiliary (e.g.,,?) preposition (e.g.,?) number (e.g.,,) negative (e.g.,X,:) determiner (e.g.,?,?,Y) function (e.g. ?,?) letter (English character) punctuation (e.g., # $) last name (e.g.,K) foreign name (e.g.,?) maybe last-name (e.g.,?,[) plural character (e.g.,?,?) pronoun (e.g.,fi,?,?) unit character (e.g.,G,?) country name (e.g.,?,?) Chinese place name (e.g.,?) organization name title suffix (e.g.,?,?) title prefix (e.g.,,?) date (e.g.,#,?,?) Figure 1: Lexicons used in our experiments C?2: second previous character in lexicon C?1: previous character in lexicon C1: next character in lexicon C2: second next character in lexicon C0C1: current and next character in lexicon C?1C0: current and previous character in lexicon C?2C?1: previous two characters in lexicon C?1C0C1: previous, current, and next character in the lexicon Figure 2: Feature conjunctions used in experiments use feature conjunctions in both the open and closed tests, as listed Figure 2. Since no vocabulary list could ever be complete,new word (unknown word) identification is an im portant issue in Chinese segmentation. Unknownwords cause segmentation errors in that these outof-vocabulary words in input text are often in correctly segmented into single-character or otheroverly-short words (Chen and Bai, 1998). Tradi tionally, new word detection has been considered as a standalone process. We consider here new word detection as an integral part of segmentation, aimingto improve both segmentation and new word detec tion: detected new words are added to the word list lexicon in order to improve segmentation; improved segmentation can potentially further improve new word detection. We measure the performance ofnew word detection by its improvements on seg mentation. Given a word segmentation proposed by the CRF, we can compute a confidence in each segment. We detect as new words those that are not in the existing word list, yet are either highly confident segments, or low confident segments that are surrounded by high confident words. A confidence threshold of 0.9 is determined by cross-validation.Segment confidence is estimated using constrained forward-backward (Culotta and McCallum, 2004). The standard forward-backward algorithm (Rabiner, 1990) calculates Zx, the total like lihood of all label sequences y given a sequence x. Constrained forward-backward algorithm calculates Z ?x, total likelihood of all paths passing through a constrained segment (in our case, a sequence of characters starting with a START tag followed by a few NONSTART tags before the next START tag). The confidence in this segment is then Z ? x Zx , a real number between 0 and 1.In order to increase recall of new words, we consider not only the most likely (Viterbi) segmen tation, but the segmentations in the top N most likely segmentations (an N -best list), and detect new words according to the above criteria in all N segmentations.Many errors can be corrected by new word detection. For example, person name ????? hap pens four times. In the first pass of segmentation, two of them are segmented correctly and the other two are mistakenly segmented as ?? (they are segmented differently because Viterbi algorithm decodes based on context.). However, ????? is identified as a new word and added to the word list lexicon. In the second pass of segmentation, the other two mistakes are corrected. To make a comprehensive evaluation, we use allfour of the datasets from a recent Chinese word segmentation bake-off competition (Sproat and Emer son, 2003). These datasets represent four different segmentation standards. A summary of the datasets is shown in Table 1. The standard bake-off scoring program is used to calculate precision, recall, F1, and OOV word recall. 5.1 Experimental design. Since CTB and PK are provided in the GB encod ing while AS and HK use the Big5 encoding, we convert AS and HK datasets to GB in order to make cross-training-and-testing possible. Note that this conversion could potentially worsen performance slightly due to a few conversion errors. We use cross-validation to choose Markov-order and perform feature selection. Thus, each training set is randomly split?80% used for training and theremaining 20% for validation?and based on vali dation set performance, choices are made for model structure, prior, and which word lexicons to include. The choices of prior and model structure shown in Table 2 are used for our final testing. We conduct closed and open tests on all four datasets. The closed tests use only material from the training data for the particular corpus being tested.Open tests allows using other material, such as lexicons from Internet. In open tests, we use lexi cons obtained from various resources as described Corpus Abbrev. Encoding #Train words #Test Words OOV rate (%) UPenn Chinese Treebank CTB GB 250K 40K 18.1 Beijing University PK GB 1.1M 17K 6.9 Hong Kong City U HK Big 5 240K 35K 7.1 Academia Sinica AS Big 5 5.8M 12K 2.2 Table 1: Datasets statistics bin-Size M Markov order CTB 10 first-order + transitions PK 15 first-order + transitions HK 1 first-order AS 15 first-order + transitions Table 2: Optimal prior and Markov order setting in Section 3.1. In addition, we conduct cross-dataset tests, in which we train on one dataset and test on other datasets. 5.2 Overall results. Final results of CRF based segmentation with newword detection are summarized in Table 3. The up per part of the table contains the closed test results, and the lower part contains the open test results. Each entry is the performance of the given metric (precision, recall, F1, and Roov) on the test set. Closed Precision Recall F1 Roov CTB 0.828 0.870 0.849 0.550 PK 0.935 0.947 0.941 0.660 HK 0.917 0.940 0.928 0.531 AS 0.950 0.962 0.956 0.292 Open Precision Recall F1 Roov CTB 0.889 0.898 0.894 0.619 PK 0.941 0.952 0.946 0.676 HK 0.944 0.948 0.946 0.629 AS 0.953 0.961 0.957 0.403 Table 3: Overall results of CRF segmentation on closed and open tests To compare our results against other systems, we summarize the competition results reported in (Sproat and Emerson, 2003) in Table 4. XXc and XXo indicate the closed and open runs on datasetXX respectively. Entries contain the F1 perfor mance of each participating site on different runs, with the best performance in bold. Our results are in the last row. Column SITE-AVG is the averageF1 performance over the datasets on which a site re ported results. Column OUR-AVG is the average F1 performance of our system over the same datasets.Comparing performance across systems is diffi cult since none of those systems reported results on all eight datasets (open and closed runs on 4 datasets). Nevertheless, several observations could be made from Table 4. First, no single system achieved best results in all tests. Only one site (S01)achieved two best runs (CTBc and PKc) with an av erage of 91.8% over 6 runs. S01 is one of the best segmentation systems in mainland China (Zhang et al., 2003). We also achieve two best runs (ASo and HKc), with a comparable average of 91.9% over the same 6 runs, and a 92.7% average over all the 8 runs.Second, performance varies significantly across dif ferent datasets, indicating that the four datasets havedifferent characteristics and use very different seg mentation guidelines. We also notice that the worstresults were obtained on CTB dataset for all systems. This is due to significant inconsistent segmen tation in training and testing (Sproat and Emerson, 2003). We verify this by another test. We randomly split the training data into 80% training and 20%testing, and run the experiments for 3 times, result ing in a testing F1 of 97.13%. Third, consider a comparison of our results with site S12, who use a sliding-window maximum entropy model (Xue, 2003). They participated in two datasets, with an average of 93.8%. Our average over the same two runs is 94.2%. This gives some empirical evidenceof the advantages of linear-chain CRFs over sliding window maximum entropy models, however, this comparison still requires further investigation sincethere are many factors that could affect the performance such as different features used in both sys tems. To further study the robustness of our approach to segmentation, we perform cross-testing?that is, training on one dataset and testing on other datasets. Table 5 summarizes these results, in which the rows are the training datasets and the columns are thetesting datasets. Not surprisingly, cross testing re sults are worse than the results using the same ASc ASo CTBc CTBo HKc HKo PKc PKo SITE-AVG OUR-AVG S01 93.8 88.1 88.1 90.1 95.1 95.3 91.8 91.9 S02 87.4 91.2 89.3 87.2 S03 87.2 82.9 88.6 92.5 87.8 93.6 S04 93.9 93.7 93.8 94.4 S05 94.2 73.2 89.4 85.6 91.5 S06 94.5 82.9 92.4 92.4 90.6 91.9 S07 94.0 94.0 94.6 S08 90.4 95.6 93.6 93.8 93.4 94.0 S09 96.1 94.6 95.4 94.9 S10 83.1 90.1 94.7 95.9 91.0 90.8 S11 90.4 88.4 87.9 88.6 88.8 93.6 S12 95.9 91.6 93.8 94.2 95.6 95.7 84.9 89.4 92.8 94.6 94.1 94.6 92.7 Table 4: Comparisons against other systems: the first column contains the 12 sites participating in bake-off competition; the second to the ninth columns contain their results on the 8 runs, where a bold entry is the winner of that run; column SITE-AVG contains the average performance of the site over the runs in which it participated, where a bold entry indicates that this site performs better than our system; column OUR-AVG is the average of our system over the same runs, where a bolded entry indicates our system performs better than the other site; the last row is the performance of our system over all the runs and the overall average. source as training due to different segmentationpolicies, with an exception on CTB where mod els trained on other datasets perform better than the model trained on CTB itself. This is due to the data problem mentioned above. Overall, CRFs perform robustly well across all datasets. From both Table 3 and 5, we see, as expected,improvement from closed tests to open tests, indicating the significant contribution of domain knowl edge lexicons. Closed CTB PK HK AS CTB 0.822 0.810 0.815 PK 0.816 0.824 0.830 HK 0.790 0.807 0.825 AS 0.890 0.844 0.864 Open CTB PK HK AS CTB 0.863 0.870 0.894 PK 0.852 0.862 0.871 HK 0.861 0.871 0.889 AS 0.898 0.867 0.871 Table 5: Crossing test of CRF segmentation 5.3 Effects of new word detection. Table 6 shows the effect of new word detection on the closed tests. An interesting observation is CTB PK HK AS w/o NWD 0.792 0.934 0.916 0.956 NWD 0.849 0.941 0.928 0.946 Table 6: New word detection effects: w/o NWD is the results without new word detection and NWD is the results with new word detection. that the improvement is monotonically related to the OOV rate (OOV rates are listed in Table 1). This is desirable because new word detection is most needed in situations that have high OOV rate. At low OOV rate, noisy new word detection can result in worse performance, as seen in the AS dataset. 5.4 Error analysis and discussion. Several typical errors are observed in error anal ysis. One typical error is caused by inconsistent segmentation labeling in the test set. This is mostnotorious in CTB dataset. The second most typical error is in new, out-of-vocabulary words, especially proper names. Although our new word detec tion fixes many of these problems, it is not effectiveenough to recognize proper names well. One solution to this problem could use a named entity ex tractor to recognize proper names; this was found to be very helpful in Wu (2003). One of the most attractive advantages of CRFs (and maximum entropy models in general) is its the flexibility to easily incorporate arbitrary features,here in the form domain-knowledge-providing lex icons. However, obtaining these lexicons is not a trivial matter. The quality of lexicons can affect the performance of CRFs significantly. In addition, compared to simple models like n-gram language models (Teahan et al, 2000), another shortcomingof CRF-based segmenters is that it requires signifi cantly longer training time. However, training is a one-time process, and testing time is still linear in the length of the input. The contribution of this paper is three-fold. First, we apply CRFs to Chinese word segmentation and find that they achieve state-of-the art performance.Second, we propose a probabilistic new word de tection method that is integrated in segmentation, and show it to improve segmentation performance. Third, as far as we are aware, this is the first work to comprehensively evaluate on the four benchmark datasets, making a solid baseline for future research on Chinese word segmentation. AcknowledgmentsThis work was supported in part by the Center for Intelligent Information Retrieval, in part by The Cen tral Intelligence Agency, the National Security Agencyand National Science Foundation under NSF grant #IIS 0326249, and in part by SPAWARSYSCEN-SD grant number N66001-02-1-8903.\",\n          \"SemEval-2007 Task 19: Frame Semantic Structure Extraction This task consists of recognizing words and phrases that evoke semantic frames as defined in the FrameNet project (http: //framenet.icsi.berkeley.edu), and their semantic dependents, which are usually, but not always, their syntacticdependents (including subjects). The train ing data was FN annotated sentences. In testing, participants automatically annotated three previously unseen texts to match goldstandard (human) annotation, including pre dicting previously unseen frames and roles. Precision and recall were measured both for matching of labels of frames and FEs and for matching of semantic dependency trees based on the annotation. The task of labeling frame-evoking words with ap propriate frames is similar to WSD, while the task of assigning frame elements is called Semantic Role Labeling (SRL), and has been the subject of several shared tasks at ACL and CoNLL. For example, in the sentence ?Matilde said, ?I rarely eat rutabaga,?? said evokes the Statement frame, and eat evokes the Ingestion frame. The role of SPEAKER in the Statement frame is filled by Matilda, and the roleof MESSAGE, by the whole quotation. In the Inges tion frame, I is the INGESTOR and rutabaga fills theINGESTIBLES role. Since the ingestion event is con tained within the MESSAGE of the Statement event, we can represent the fact that the message conveyed was about ingestion, just by annotating the sentence with respect to these two frames. After training on FN annotations, the participants? systems labeled three new texts automatically. The evaluation measured precision and recall for frames and frame elements, with partial credit for incorrect but closely related frames. Two types of evaluation were carried out: Label matching evaluation, in which the participant?s labeled data was compareddirectly with the gold standard labeled data, and Se mantic dependency evaluation, in which both thegold standard and the submitted data were first converted to semantic dependency graphs in XML for mat, and then these graphs were compared. There are three points that make this task harder and more interesting than earlier SRL tasks: (1) while previous tasks focused on role assignment, the current task also comprises the identification of the appropriate FrameNet frame, similar to WSD, (2)the task comprises not only the labeling of individual predicates and their arguments, but also the integration of all labels into an overall semantic depen dency graph, a partial semantic representation of the overall sentence meaning based on frames and roles, and (3) the test data includes occurrences of frames that are not seen in the training data. For these cases, participant systems have to identify theclosest known frame. This is a very realistic scenario, encouraging the development of robust systems showing graceful degradation in the face of un known events. 99 The basic concept of Frame Semantics is that many words are best understood as part of a group of terms that are related to a particular type of eventand the participants and ?props? involved in it (Fill more, 1976; Fillmore, 1982). The classes of events are the semantic frames of the lexical units (LUs) that evoke them, and the roles associated with the event are referred to as frame elements (FEs). The same type of analysis applies not only to events butalso to relations and states; the frame-evoking expressions may be single words or multi-word ex pressions, which may be of any syntactic category. Note that these FE names are quite frame-specific; generalizations over them are expressed via explicit FE-FE relations. The Berkeley FrameNet project (hereafter FN) (Fillmore et al, 2003) is creating a computer- and human-readable lexical resource for English, based on the theory of frame semantics and supported by corpus evidence. The current release (1.3) of the FrameNet data, which has been freely available for instructional and research purposes since the fall of 2006, includes roughly 780 frames with roughly 10,000 word senses (lexical units). It also contains roughly 150,000 annotation sets, of which 139,000are lexicographic examples, with each sentence an notated for a single predicator. The remainder are from full-text annotation in which each sentence isannotated for all predicators; 1,700 sentences are annotated in the full-text portion of the database, ac counting for roughly 11,700 annotation sets, or 6.8 predicators (=annotation sets) per sentence. Nearly all of the frames are connected into a single graph by frame-to-frame relations, almost all of which have associated FE-to-FE relations (Fillmore et al, 2004a) 2.1 Frame Semantics of texts. The ultimate goal is to represent the lexical se mantics of all the sentences in a text, based onthe relations between predicators and their depen dents, including both phrases and clauses, which may, in turn, include other predicators; although this has been a long-standing goal of FN (Fillmore and Baker, 2001), automatic means of doing this are only now becoming available. Consider a sentence from one of the testing texts: (1) This geography is important in understanding Dublin. In the frame semantic analysis of this sentence, there are two predicators which FN has analyzed: important and understanding, as well as one which we have not yet analyzed, geography. In addition,Dublin is recognized by the NER system as a loca tion. In the gold standard annotation, we have the annotation shown in (2) for the Importance frame, evoked by the target important, and the annotationshown in (3) for the Grasp frame, evoked by under standing.(2) [FACTOR This geography] [COP is] IMPOR-TANT [UNDERTAKING in understanding Dublin].[INTERESTED PARTY INI](3) This geography is important in UNDER STANDING [PHENOMENON Dublin]. [COGNIZERCNI] The definitions of the two frames begin like this: Importance: A FACTOR affects the outcome of anUNDERTAKING, which can be a goal-oriented activ ity or the maintenance of a desirable state, the work in a FIELD, or something portrayed as affecting an INTERESTED PARTY. Grasp: A COGNIZER possesses knowledge about the workings, significance, or meaning of an idea or object, which we call PHENOMENON, and is able to make predictions about the behavior or occurrence of the PHENOMENON. Using these definitions and the labels, and the fact that the target and FEs of one frame are subsumedby an FE of the other, we can compose the meanings of the two frames to produce a detailed para phrase of the meaning of the sentence: Something denoted by this geography is a factor which affects the outcome of the undertaking of understanding the location called ?Dublin? by any interested party. We have not dealt with geography as a frame-evoking expression, although we would eventually like to. (The preposition in serves only as a marker of the frame element UNDERTAKING.) In (2), the INTERESTED PARTY is not a label onany part of the text; rather, it is marked INI, for ?indefinite null instantiation?, meaning that it is con ceptually required as part of the frame definition, absent from the sentence, and not recoverable from the context as being a particular individual?meaning 100that this geography is important for anyone in general?s understanding of Dublin. In (3), the COG NIZER is ?constructionally null instantiated?, as thegerund understanding licenses omission of its sub ject. The marking of null instantiations is important in handling text coherence and was part of the goldstandard, but as far as we know, none of the participants attempted it, and it was ignored in the evalua tion.Note that we have collapsed the two null instantiated FEs, the INTERESTED PARTY of the impor tance frame and the COGNIZER in the Grasp frame, since they are not constrained to be distinct. 2.2 Semantic dependency graphs. Since the role fillers are dependents (broadly speak ing) of the predicators, the full FrameNet annotation of a sentence is roughly equivalent to a dependency parse, in which some of the arcs are labeled with rolenames; and a dependency graph can be derived algorithmically from FrameNet annotation; an early ver sion of this was proposed by (Fillmore et al, 2004b)Fig. 1 shows the semantic dependency graph derived from sentence (1); this graphical representa tion was derived from a semantic dependency XML file (see Sec. 5). It shows that the top frame in this sentence is evoked by the word important, although the syntactic head is the copula is (here given the more general label ?Support?). The labels on thearcs are either the names of frame elements or indications of which of the daughter nodes are seman tic heads, which is important in some versions of the evaluation. The labels on nodes are either frame names (also colored gray), syntactic phrases types (e.g. NP), or the names of certain other syntactic ?connectors?, in this case, Marker and Support. 3.1 Training data. The major part of the training data for the task con sisted of the current data release from FrameNet(Release 1.3), described in Sec.2 This was supple mented by additional training data made availablethrough SemEval to participants in this task. In addition to updated versions of some of the full-text an notation from Release 1.3, three files from the ANC were included: from Slate.com, ?Stephanopoulos Importance: important Marker: in Undertaking NP Factor Grasp: understanding SemHead This geography Head NE:location: Dublin DenotedFE: location Phenomenon <s> Supp: is Head . SemHead Figure 1: Sample Semantic Dependency Graph Crimes? and ?Entrepreneur as Madonna?, and from the Berlitz travel guides, ?History of Jerusalem?. 3.2 Testing data. The testing data was made up of three texts, none of which had been seen before; the gold standard consisted of manual annotations (by the FrameNetteam) of these texts for all frame evoking expressions and the fillers of the associated frame elements. All annotation of the testing data was carefully reviewed by the FN staff to insure its cor rectness. Since most of the texts annotated in the FN database are from the NTI website (www.nti.org), we decided to take two of the three test ing texts from there also. One, ?China Overview?, was very similar to other annotated texts such as ?Taiwan Introduction?, ?Russia Overview?, etc. available in Release 1.3. The other NTI text, ?Work Advances?, while in the same domain, was shorter and closer to newspaper style than the rest of the NTI texts. Finally, the ?Introduction to 101 Sents NEs Frames Tokens Types Work 14 31 174 77 China 39 90 405 125 Dublin 67 86 480 165 Totals 120 207 1059 272 Table 1: Summary of Testing DataDublin?, taken from the American National Cor pus (ANC, www.americannationalcorpus. org) Berlitz travel guides, is of quite a different genre, although the ?History of Jerusalem? text in the training data was somewhat similar. Table 1 gives some statistics on the three testing files. To give a flavor of the texts, here are two sentences; frame evoking words are in boldface: From ?Work Advances?: ?The Iranians are now willing to accept the installation of cameras only outside the cascade halls, which will not enable the IAEA to monitor the entire uranium enrichment process,? the diplomat said. From ?Introduction to Dublin?: And in this city, where literature and theater have historicallydominated the scene, visual arts are finally com ing into their own with the new Museum of Modern Art and the many galleries that display the work of modern Irish artists. A number of groups downloaded the training or test ing data, but in the end, only three groups submitted results: the UTD-SRL group and the LTH group, who submitted full results, and the CLR group who submitted results for frames only. It should also be noted that the LTH group had the testing data for longer than the 10 days allowed by the rules of the exercise, which means that the results of the two teams are not exactly comparable. Also, the results from the CLR group were initially formatted slightly differently from the gold standard with regard to character spacing; a later reformatting allowed their results to be scored with the other groups?. The LTH system used only SVM classifiers, while the UTD-SRL system used a combination of SVM and ME classifiers, determined experimentally. The CLR system did not use classifiers, but hand-written symbolic rules. Please consult the separate system papers for details about the features used. The labels-only matching was similar to previousshared tasks, but the dependency structure evaluation deserves further explanation: The XML seman tic dependency structure was produced by a program called fttosem, implemented in Perl, which goes sentence by sentence through a FrameNet full-text XML file, taking LU, FE, and other labels and using them to structure a syntactically unparsed piece of a sentence into a syntactic-semantic tree. Two basic principles allow us to produce this tree: (1) LUs are the sole syntactic head of a phrase whose semantics is expressed by their frame and (2) each label span is interpreted as the boundaries of a syntactic phrase, so that when a larger label span subsumes a smaller one, the larger span can be interpreted as a the highernode in a hierarchical tree. There are a fair num ber of complications, largely involving identifyingmismatches between syntactic and semantic headedness. Some of these (support verbs, copulas, modifiers, transparent nouns, relative clauses) are annotated in the data with their own labels, while others (syntactic markers, e.g. prepositions, and auxil iary verbs) must be identified using simple syntactic heuristics and part-of-speech tags. For this evaluation, a non-frame node counts as matching provided that it includes the head of the gold standard, whether or not non-head children ofthat node are included. For frame nodes, the partici pants got full credit if the frame of the node matched the gold standard. 5.1 Partial credit for related frames. One of the problems inherent in testing against un seen data is that it will inevitably contain lexical units that have not previously been annotated in FrameNet, so that systems which do not generalizewell cannot get them right. In principle, the deci sion as to what frame to add a new LU to should be helped by the same criteria that are used to assign polysemous lemmas to existing frames. However,in practice this assignment is difficult, precisely be cause, unlike WSD, there is no assumption that all the senses of each lemma are defined in advance; if 102 the system can?t be sure that a new use of a lemma is in one of the frames listed for that lemma, thenit must consider all the 800+ frames as possibili ties. This amounts to the automatic induction of fine-grained semantic similarity from corpus data, a notoriously difficult problem (Stevenson and Joanis, 2003; Schulte im Walde, 2003).For LUs which clearly do not fit into any exist ing frames, the problem is still more difficult. In the course of creating the gold standard annotation of the three testing texts, the FN team created almost 40 new frames. We cannot ask that participants hit uponthe new frame name, but the new frames are not created in a vacuum; as mentioned above, they are almost always added to the existing structure of frame to-frame relations; this allows us to give credit for assignment to frames which are not the precise onein the gold standard, but are close in terms of frame to-frame relations. Whenever participants? proposed frames were wrong but connected to the right frameby frame relations, partial credit was given, decreas ing by 20% for each link in the frame-frame relationgraph between the proposed frame and the gold stan dard. For FEs, each frame element had to match the gold standard frame element and contain at least the same head word in order to gain full credit; again, partial credit was given for frame elements related via FE-to-FE relations. Text Group Recall Prec. F1 Dublin UTD-SRL 0.4188 0.7716 0.5430 China UTD-SRL 0.5498 0.8009 0.6520 Work UTD-SRL 0.5251 0.8382 0.6457 Dublin LTH 0.5184 0.7156 0.6012 China LTH 0.6261 0.7731 0.6918 Work LTH 0.6606 0.8642 0.7488 Dublin CLR 0.3984 0.6469 0.4931 China CLR 0.4621 0.6302 0.5332 Work CLR 0.5054 0.7452 0.6023 Table 2: Frame Recognition onlyThe strictness of the requirement of exact boundary matching (which depends on an accurate syntac tic parse) is compounded by the cascading effect of semantic classification errors, as seen by comparing Text Group Recall Prec. F1 Label matching only Dublin UTD-SRL 0.27699 0.55663 0.36991 China UTD-SRL 0.31639 0.51715 0.39260 Work UTD-SRL 0.31098 0.62408 0.41511 Dublin LTH 0.36536 0.55065 0.43926 China LTH 0.39370 0.54958 0.45876 Work LTH 0.41521 0.61069 0.49433 Semantic dependency matching Dublin UTD-SRL 0.26238 0.53432 0.35194 China UTD-SRL 0.31489 0.53145 0.39546 Work UTD-SRL 0.30641 0.61842 0.40978 Dublin LTH 0.36345 0.54857 0.43722 China LTH 0.40995 0.57410 0.47833 Work LTH 0.45970 0.67352 0.54644Table 3: Results for combined Frame and FE recog nition the F-scores in Table 3 with those in Table 2. The difficulty of the task is reflected in the F-scores of around 35% for the most difficult text in the most difficult condition, but participants still managed to reach F-scores as high as 75% for the more limited task of Frame Identification (Table 2), which more closely matches traditional Senseval tasks, despite the lack of a full sense inventory. The difficulty posed by having such an unconstrained task led to understandably low recall scores in all participants (between 25 and 50%). The systems submitted by the teams differed in their sensitivity to differences in the texts: UTD-SRL?s system varied by around 10% across texts, while LTH?s varied by 15%. There are some rather encouraging results also.The participants rather consistently performed bet ter with our more complex, but also more useful andrealistic scoring, including partial credit and grad ing on semantic dependency rather than exact span match (compare the top and bottom halves of Table 3). The participants all performed relatively well onthe frame-recognition task, with precision scores av eraging 63% and topping 85%. The testing data for this task turned out to be espe cially challenging with regard to new frames, since, in an effort to annotate especially thoroughly, almost 10340 new frames were created in the process of an notating these three specific passages. One result of this was that the test passages had more unseenframes than a random unseen passage, which prob ably lowered the recall on frames. It appears that this was not entirely compensated by giving partial credit for related frames. This task is a more advanced and realistic version of the Automatic Semantic Role Labeling task of Senseval-3 (Litkowski, 2004). Unlike that task, the testing data was previously unseen, participants had to determine the correct frames as a first step, and participants also had to determine FE boundaries, which were given in the Senseval-3. A crucial difference from similar approaches, such as SRL with PropBank roles (Pradhan et al, 2004) is that by identifying relations as part of a frame, you have identified a gestalt of relations that enables far more inference, and sentences from the same passage that use other words from the same frame will be easier to link together. Thus, the FN SRL results are translatable fairly directly intoformal representations which can be used for rea soning, question answering, etc. (Scheffczyk et al., 2006; Frank and Semecky, 2004; Sinha and Narayanan, 2005). Despite the problems with recall, the participants have expressed a determination to work to improvethese results, and the FN staff are eager to collabo rate in this effort. A project is now underway at ICSI to speed up frame and LU definition, and another tospeed up the training of SRL systems is just begin ning, so the prospects for improvement seem good.This material is based in part upon work sup ported by the National Science Foundation under Grant No. IIS-0535297.\",\n          \"Target-dependent Twitter Sentiment Classification Sentiment analysis on Twitter data has attracted much attention recently. In this paper, we focus on target-dependent Twitter sentiment classification; namely, given a query, we classify the sentiments of the tweets as positive, negative or neutral according to whether they contain positive, negative or neutral sentiments about that query. Here the query serves as the target of the sentiments. The state-ofthe-art approaches for solving this problem always adopt the target-independent strategy, which may assign irrelevant sentiments to the given target. Moreover, the state-of-the-art approaches only take the tweet to be classified into consideration when classifying the sentiment; they ignore its context (i.e., related tweets). However, because tweets are usually short and more ambiguous, sometimes it is not enough to consider only the current tweet for sentiment classification. In this paper, we propose to improve target-dependent Twitter sentiment classification by 1) incorporating target-dependent features; and 2) taking related tweets into consideration. According to the experimental results, our approach greatly improves the performance of target-dependent sentiment classification. Twitter, as a micro-blogging system, allows users to publish tweets of up to 140 characters in length to tell others what they are doing, what they are thinking, or what is happening around them. Over the past few years, Twitter has become very popular. According to the latest Twitter entry in Wikipedia, the number of Twitter users has climbed to 190 million and the number of tweets published on Twitter every day is over 65 million1. As a result of the rapidly increasing number of tweets, mining people\\u2019s sentiments expressed in tweets has attracted more and more attention. In fact, there are already many web sites built on the Internet providing a Twitter sentiment search service, such as Tweetfeel2, Twendz3, and Twitter Sentiment4. In those web sites, the user can input a sentiment target as a query, and search for tweets containing positive or negative sentiments towards the target. The problem needing to be addressed can be formally named as Target-dependent Sentiment Classification of Tweets; namely, given a query, classifying the sentiments of the tweets as positive, negative or neutral according to whether they contain positive, negative or neutral sentiments about that query. Here the query serves as the target of the sentiments. The state-of-the-art approaches for solving this problem, such as (Go et al., 20095; Barbosa and Feng, 2010), basically follow (Pang et al., 2002), who utilize machine learning based classifiers for the sentiment classification of texts. However, their classifiers actually work in a target-independent way: all the features used in the classifiers are independent of the target, so the sentiment is decided no matter what the target is. Since (Pang et al., 2002) (or later research on sentiment classification of product reviews) aim to classify the polarities of movie (or product) reviews and each movie (or product) review is assumed to express sentiments only about the target movie (or product), it is reasonable for them to adopt the target-independent approach. However, for target-dependent sentiment classification of tweets, it is not suitable to exactly adopt that approach. Because people may mention multiple targets in one tweet or comment on a target in a tweet while saying many other unrelated things in the same tweet, target-independent approaches are likely to yield unsatisfactory results: In fact, it is easy to find many such cases by looking at the output of Twitter Sentiment or other Twitter sentiment analysis web sites. Based on our manual evaluation of Twitter Sentiment output, about 40% of errors are because of this (see Section 6.1 for more details). In addition, tweets are usually shorter and more ambiguous than other sentiment data commonly used for sentiment analysis, such as reviews and blogs. Consequently, it is more difficult to classify the sentiment of a tweet only based on its content. For instance, for the following tweet, which contains only three words, it is difficult for any existing approaches to classify its sentiment correctly. However, relations between individual tweets are more common than those in other sentiment data. We can easily find many related tweets of a given tweet, such as the tweets published by the same person, the tweets replying to or replied by the given tweet, and retweets of the given tweet. These related tweets provide rich information about what the given tweet expresses and should definitely be taken into consideration for classifying the sentiment of the given tweet. In this paper, we propose to improve targetdependent sentiment classification of tweets by using both target-dependent and context-aware approaches. Specifically, the target-dependent approach refers to incorporating syntactic features generated using words syntactically connected with the given target in the tweet to decide whether or not the sentiment is about the given target. For instance, in the second example, using syntactic parsing, we know that \\u201cWindows 7\\u201d is connected to \\u201cbetter\\u201d by a copula, while \\u201cVista\\u201d is connected to \\u201cbetter\\u201d by a preposition. By learning from training data, we can probably predict that \\u201cWindows 7\\u201d should get a positive sentiment and \\u201cVista\\u201d should get a negative sentiment. In addition, we also propose to incorporate the contexts of tweets into classification, which we call a context-aware approach. By considering the sentiment labels of the related tweets, we can further boost the performance of the sentiment classification, especially for very short and ambiguous tweets. For example, in the third example we mentioned above, if we find that the previous and following tweets published by the same person are both positive about the Lakers, we can confidently classify this tweet as positive. The remainder of this paper is structured as follows. In Section 2, we briefly summarize related work. Section 3 gives an overview of our approach. We explain the target-dependent and contextaware approaches in detail in Sections 4 and 5 respectively. Experimental results are reported in Section 6 and Section 7 concludes our work. In recent years, sentiment analysis (SA) has become a hot topic in the NLP research community. A lot of papers have been published on this topic. Specifically, Turney (2002) proposes an unsupervised method for classifying product or movie reviews as positive or negative. In this method, sentimental phrases are first selected from the reviews according to predefined part-of-speech patterns. Then the semantic orientation score of each phrase is calculated according to the mutual information values between the phrase and two predefined seed words. Finally, a review is classified based on the average semantic orientation of the sentimental phrases in the review. In contrast, (Pang et al., 2002) treat the sentiment classification of movie reviews simply as a special case of a topic-based text categorization problem and investigate three classification algorithms: Naive Bayes, Maximum Entropy, and Support Vector Machines. According to the experimental results, machine learning based classifiers outperform the unsupervised approach, where the best performance is achieved by the SVM classifier with unigram presences as features. Besides the above mentioned work for targetindependent sentiment classification, there are also several approaches proposed for target-dependent classification, such as (Nasukawa and Yi, 2003; Hu and Liu, 2004; Ding and Liu, 2007). (Nasukawa and Yi, 2003) adopt a rule based approach, where rules are created by humans for adjectives, verbs, nouns, and so on. Given a sentiment target and its context, part-of-speech tagging and dependency parsing are first performed on the context. Then predefined rules are matched in the context to determine the sentiment about the target. In (Hu and Liu, 2004), opinions are extracted from product reviews, where the features of the product are considered opinion targets. The sentiment about each target in each sentence of the review is determined based on the dominant orientation of the opinion words appearing in the sentence. As mentioned in Section 1, target-dependent sentiment classification of review sentences is quite different from that of tweets. In reviews, if any sentiment is expressed in a sentence containing a feature, it is very likely that the sentiment is about the feature. However, the assumption does not hold in tweets. As Twitter becomes more popular, sentiment analysis on Twitter data becomes more attractive. (Go et al., 2009; Parikh and Movassate, 2009; Barbosa and Feng, 2010; Davidiv et al., 2010) all follow the machine learning based approach for sentiment classification of tweets. Specifically, (Davidiv et al., 2010) propose to classify tweets into multiple sentiment types using hashtags and smileys as labels. In their approach, a supervised KNN-like classifier is used. In contrast, (Barbosa and Feng, 2010) propose a two-step approach to classify the sentiments of tweets using SVM classifiers with abstract features. The training data is collected from the outputs of three existing Twitter sentiment classification web sites. As mentioned above, these approaches work in a target-independent way, and so need to be adapted for target-dependent sentiment classification. The problem we address in this paper is targetdependent sentiment classification of tweets. So the input of our task is a collection of tweets containing the target and the output is labels assigned to each of the tweets. Inspired by (Barbosa and Feng, 2010; Pang and Lee, 2004), we design a three-step approach in this paper: In each of the first two steps, a binary SVM classifier is built to perform the classification. To train the classifiers, we use SVM-Light6 with a linear kernel; the default setting is adopted in all experiments. In our approach, rich feature representations are used to distinguish between sentiments expressed towards different targets. In order to generate such features, much NLP work has to be done beforehand, such as tweet normalization, POS tagging, word stemming, and syntactic parsing. In our experiments, POS tagging is performed by the OpenNLP POS tagger7. Word stemming is performed by using a word stem mapping table consisting of about 20,000 entries. We also built a simple rule-based model for tweet normalization which can correct simple spelling errors and variations into normal form, such as \\u201cgooood\\u201d to \\u201cgood\\u201d and \\u201cluve\\u201d to \\u201clove\\u201d. For syntactic parsing we use a Maximum Spanning Tree dependency parser (McDonald et al., 2005). Previous work (Barbosa and Feng, 2010; Davidiv et al., 2010) has discovered many effective features for sentiment analysis of tweets, such as emoticons, punctuation, prior subjectivity and polarity of a word. In our classifiers, most of these features are also used. Since these features are all generated without considering the target, we call them targetindependent features. In both the subjectivity classifier and polarity classifier, the same targetindependent feature set is used. Specifically, we use two kinds of target-independent features: Besides target-independent features, we also incorporate target-dependent features in both the subjectivity classifier and polarity classifier. We will explain them in detail below. It is quite common that people express their sentiments about a target by commenting not on the target itself but on some related things of the target. For example, one may express a sentiment about a company by commenting on its products or technologies. To express a sentiment about a product, one may choose to comment on the features or functionalities of the product. It is assumed that readers or audiences can clearly infer the sentiment about the target based on those sentiments about the related things. As shown in the tweet below, the author expresses a positive sentiment about \\u201cMicrosoft\\u201d by expressing a positive sentiment directly about \\u201cMicrosoft technologies\\u201d. \\u201cI am passionate about Microsoft technologies especially Silverlight.\\u201d In this paper, we define those aforementioned related things as Extended Targets. Tweets expressing positive or negative sentiments towards the extended targets are also regarded as positive or negative about the target. Therefore, for targetdependent sentiment classification of tweets, the first thing is identifying all extended targets in the input tweet collection. In this paper, we first regard all noun phrases, including the target, as extended targets for simplicity. However, it would be interesting to know under what circumstances the sentiment towards the target is truly consistent with that towards its extended targets. For example, a sentiment about someone\\u2019s behavior usually means a sentiment about the person, while a sentiment about someone\\u2019s colleague usually has nothing to do with the person. This could be a future work direction for target-dependent sentiment classification. In addition to the noun phrases including the target, we further expand the extended target set with the following three methods: 1. Adding mentions co-referring to the target as new extended targets. It is common that people use definite or demonstrative noun phrases or pronouns referring to the target in a tweet and express sentiments directly on them. For instance, in \\u201cOh, Jon Stewart. How I love you so.\\u201d, the author expresses a positive sentiment to \\u201cyou\\u201d which actually refers to \\u201cJon Stewart\\u201d. By using a simple co-reference resolution tool adapted from (Soon et al., 2001), we add all the mentions referring to the target into the extended target set. 2. Identifying the top K nouns and noun phrases which have the strongest association with the target. Here, we use Pointwise Mutual Information (PMI) to measure the association. Where p(w,t), p(w), and p(t) are probabilities of w and t co-occurring, w appearing, and t appearing in a tweet respectively. In the experiments, we estimate them on a tweet corpus containing 20 million tweets. We set K = 20 in the experiments based on empirical observations. 3. Extracting head nouns of all extended targets, whose PMI values with the target are above some predefined threshold, as new extended targets. For instance, suppose we have found \\u201cMicrosoft Technologies\\u201d as the extended target, we will further add \\u201ctechnologies\\u201d into the extended target set if the PMI value for \\u201ctechnologies\\u201d and \\u201cMicrosoft\\u201d is above the threshold. Similarly, we can find \\u201cprice\\u201d as the extended targets for \\u201ciPhone\\u201d from \\u201cthe price of iPhone\\u201d and \\u201cLoveGame\\u201d for \\u201cLady Gaga\\u201d from \\u201cLoveGame by Lady Gaga\\u201d. Target-dependent sentiment classification needs to distinguish the expressions describing the target from other expressions. In this paper, we rely on the syntactic parse tree to satisfy this need. Specifically, for any word stem wi in a tweet which has one of the following relations with the given target T or any from the extended target set, we generate corresponding target-dependent features with the following rules: \\u2022 wi is a transitive verb and T (or any of the extended target) is its object; we generate a feature wi _arg2. \\u201carg\\u201d is short for \\u201cargument\\u201d. For example, for the target iPhone in \\u201cI love iPhone\\u201d, we generate \\u201clove_arg2\\u201d as a feature. has T (or any of the extended target) as its subject; we generate a feature arg1_v_wi. For example, for the target iPhone in the tweet \\u201ciPhone works better with the CellBand\\u201d, we will generate the feature \\u201carg1_v_well\\u201d. Moreover, if any word included in the generated target-dependent features is modified by a negation9, then we will add a prefix \\u201cneg-\\u201d to it in the generated features. For example, for the target iPhone in the tweet \\u201ciPhone does not work better with the CellBand\\u201d, we will generate the features \\u201carg1_v_neg-well\\u201d and \\u201cneg-work_it_arg1\\u201d. To overcome the sparsity of target-dependent features mentioned above, we design a special binary feature indicating whether or not the tweet contains at least one of the above target-dependent features. Target-dependent features are binary features, each of which corresponds to the presence of the feature in the tweet. If the feature is present, the entry will be 1; otherwise it will be 0. As we mentioned in Section 1, since tweets are usually shorter and more ambiguous, it would be useful to take their contexts into consideration when classifying the sentiments. In this paper, we regard the following three kinds of related tweets as context for a tweet. Based on these three kinds of relations, we can construct a graph using the input tweet collection of a given target. As illustrated in Figure 1, each circle in the graph indicates a tweet. The three kinds of edges indicate being published by the same person (solid line), retweeting (dash line), and replying relations (round dotted line) respectively. If we consider that the sentiment of a tweet only depends on its content and immediate neighbors, we can leverage a graph-based method for sentiment classification of tweets. Specifically, the probability of a tweet belonging to a specific sentiment class can be computed with the following formula: Where c is the sentiment label of a tweet which belongs to {positive, negative, neutral}, G is the tweet graph, N(d) is a specific assignment of sentiment labels to all immediate neighbors of the tweet, and r is the content of the tweet. We can convert the output scores of a tweet by the subjectivity and polarity classifiers into probabilistic form and use them to approximate p(c |r). Then a relaxation labeling algorithm described in (Angelova and Weikum, 2006) can be used on the graph to iteratively estimate p(c|r,G) for all tweets. After the iteration ends, for any tweet in the graph, the sentiment label that has the maximum p(c |r,G) is considered the final label. Because there is no annotated tweet corpus publicly available for evaluation of target-dependent Twitter sentiment classification, we have to create our own. Since people are most interested in sentiments towards celebrities, companies and products, we selected 5 popular queries of these kinds: {Obama, Google, iPad, Lakers, Lady Gaga}. For each of those queries, we downloaded 400 English tweets10 containing the query using the Twitter API. We manually classify each tweet as positive, negative or neutral towards the query with which it is downloaded. After removing duplicate tweets, we finally obtain 459 positive, 268 negative and 1,212 neutral tweets. Among the tweets, 100 are labeled by two human annotators for inter-annotator study. The results show that for 86% of them, both annotators gave identical labels. Among the 14 tweets which the two annotators disagree on, only 1 case is a positive-negative disagreement (one annotator considers it positive while the other negative), and the other 13 are all neutral-subjective disagreement. This probably indicates that it is harder for humans to decide if a tweet is neutral or subjective than to decide if it is positive or negative. We first analyze the output of Twitter Sentiment (TS) using the five test queries. For each query, we randomly select 20 tweets labeled as positive or negative by TS. We also manually classify each tweet as positive, negative or neutral about the corresponding query. Then, we analyze those tweets that get different labels from TS and humans. Finally we find two major types of error: 1) Tweets which are totally neutral (for any target) are classified as subjective by TS; 2) sentiments in some tweets are classified correctly but the sentiments are not truly about the query. The two types take up about 35% and 40% of the total errors, respectively. The second type is actually what we want to resolve in this paper. After further checking those tweets of the second type, we found that most of them are actually neutral for the target, which means that the dominant error in Twitter Sentiment is classifying neutral tweets as subjective. Below are several examples of the second type where the bolded words are the targets. \\u201cNo debate needed, heat can't beat lakers or celtics\\u201d (negative by TS but positive by human) \\u201cwhy am i getting spams from weird people asking me if i want to chat with lady gaga\\u201d (positive by TS but neutral by human) \\u201cBringing iPhone and iPad apps into cars? http://www.speakwithme.com/ will be out soon and alpha is awesome in my car.\\u201d (positive by TS but neutral by human) \\u201cHere's a great article about Monte Veronese cheese. It's in Italian so just put the url into Google translate and enjoy http://ow.ly/3oQ77\\u201d (positive by TS but neutral by human) We conduct several experiments to evaluate subjectivity classifiers using different features. In the experiments, we consider the positive and negative tweets annotated by humans as subjective tweets (i.e., positive instances in the SVM classifiers), which amount to 727 tweets. Following (Pang et al., 2002), we balance the evaluation data set by randomly selecting 727 tweets from all neutral tweets annotated by humans and consider them as objective tweets (i.e., negative instances in the classifiers). We perform 10-fold cross-validations on the selected data. Following (Go et al., 2009; Pang et al., 2002), we use accuracy as a metric in our experiments. The results are listed below. As shown in Table 1, the classifier using only the content features achieves an accuracy of 61.1%. Adding sentiment lexicon features improves the accuracy to 63.8%. Finally, the best performance (68.2%) is achieved by combining targetdependent features and other features (t-test: p < 0.005). This clearly shows that target-dependent features do help remove many sentiments not truly about the target. We also re-implemented the method proposed in (Barbosa and Feng, 2010) for comparison. From Table 1, we can see that all our systems perform better than (Barbosa and Feng, 2010) on our data set. One possible reason is that (Barbosa and Feng, 2010) use only abstract features while our systems use more lexical features. To further evaluate the contribution of target extension, we compare the system using the exact target and all extended targets with that using only the exact target. We also eliminate the extended targets generated by each of the three target extension methods and reevaluate the performances. As shown in Table 2, without extended targets, the accuracy is 65.6%, which is still higher than those using only target-independent features. After adding all extended targets, the accuracy is improved significantly to 68.2% (p < 0.005), which suggests that target extension does help find indirectly expressed sentiments about the target. In addition, all of the three methods contribute to the overall improvement, with the head noun method contributing most. However, the other two methods do not contribute significantly. Similarly, we conduct several experiments on positive and negative tweets to compare the polarity classifiers with different features, where we use 268 negative and 268 randomly selected positive tweets. The results are listed below. From Table 3, we can see that the classifier using only the content features achieves the worst accuracy (78.8%). Sentiment lexicon features are shown to be very helpful for improving the performance. Similarly, we re-implemented the method proposed by (Barbosa and Feng, 2010) in this experiment. The results show that our system using both content features and sentiment lexicon features performs slightly better than (Barbosa and Feng, 2010). The reason may be same as that we explained above. Again, the classifier using all features achieves the best performance. Both the classifiers with all features and with the combination of content and sentiment lexicon features are significantly better than that with only the content features (p < 0.01). However, the classifier with all features does not significantly outperform that using the combination of content and sentiment lexicon features. We also note that the improvement by target-dependent features here is not as large as that in subjectivity classification. Both of these indicate that targetdependent features are more useful for improving subjectivity classification than for polarity classification. This is consistent with our observation in Subsection 6.2 that most errors caused by incorrect target association are made in subjectivity classification. We also note that all numbers in Table 3 are much bigger than those in Table 1, which suggests that subjectivity classification of tweets is more difficult than polarity classification. Similarly, we evaluated the contribution of target extension for polarity classification. According to the results, adding all extended targets improves the accuracy by about 1 point. However, the contributions from the three individual methods are not statistically significant. As seen in Figure 1, there are several tweets which are not connected with any other tweets. For these tweets, our graph-based optimization approach will have no effect. The following table shows the percentages of the tweets in our evaluation data set which have at least one related tweet according to various relation types. According to Table 4, for 66.2% of the tweets concerning the test queries, we can find at least one related tweet. That means our context-aware approach is potentially useful for most of the tweets. To evaluate the effectiveness of our contextaware approach, we compared the systems with and without considering the context. As shown in Table 5, the overall accuracy of the target-dependent classifiers over three classes is 66.0%. The graph-based optimization improves the performance by over 2 points (p < 0.005), which clearly shows that the context information is very useful for classifying the sentiments of tweets. From the detailed improvement for each sentiment class, we find that the context-aware approach is especially helpful for positive and negative classes. We further compared the three types of relations for context-aware sentiment classification; the results are reported in Table 6. Clearly, being published by the same person is the most useful relation for sentiment classification, which is consistent with the percentage distribution of the tweets over relation types; using retweet only does not help. One possible reason for this is that the retweets and their original tweets are nearly the same, so it is very likely that they have already got the same labels in previous classifications. Twitter sentiment analysis has attracted much attention recently. In this paper, we address targetdependent sentiment classification of tweets. Different from previous work using targetindependent classification, we propose to incorporate syntactic features to distinguish texts used for expressing sentiments towards different targets in a tweet. According to the experimental results, the classifiers incorporating target-dependent features significantly outperform the previous targetindependent classifiers. In addition, different from previous work using only information on the current tweet for sentiment classification, we propose to take the related tweets of the current tweet into consideration by utilizing graph-based optimization. According to the experimental results, the graph-based optimization significantly improves the performance. As mentioned in Section 4.1, in future we would like to explore the relations between a target and any of its extended targets. We are also interested in exploring relations between Twitter accounts for classifying the sentiments of the tweets published by them. We would like to thank Matt Callcut for refining the language of this paper, and thank Yuki Arase and the anonymous reviewers for many valuable comments and helpful suggestions. We would also thank Furu Wei and Xiaolong Wang for their help with some of the experiments and the preparation of the camera-ready version of the paper.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1009,\n        \"samples\": [\n          \"Chinese Segmentation And New Word Detection Using Conditional Random Fields\\nChinese word segmentation is a difficult, important and widely-studied sequence modeling problem.\\nThis paper demonstrates the ability of linear-chain conditional random fields (CRFs) to perform robust and accurate Chinese word segmentation by providing a principled framework that easily supports the integration of domain knowledge in the form of multiple lexicons of characters and words.\\nWe also present a probabilistic new word detection method, which further improves performance.\\nOur system is evaluated on four datasets used in a recent comprehensive Chinese word segmentation competition.\\nState-of-the-art performance is obtained.\\nThe superiority of CRFs on Chinese information processing was also demonstrated in word segmentation).\\nCRF is a statistical sequence modeling framework introduced by Lafferty et al (2001), and we use it for the Chinese word segmentation task by treating word segmentation as a binary decision task.\\nWe first use this framework for Chinese word segmentation by treating it as a binary decision task, such that each character is labeled either as the beginning of a word or the continuation of one.\\nWe define the word segmentation problem as labeling each character as whether or not the previous character boundary of the current character is a word boundary.\\n\",\n          \"SemEval-2007 Task 19: Frame Semantic Structure Extraction\\nThis task consists of recognizing words and phrases that evoke semantic frames as defined in the FrameNet project (http://framenet.icsi.berkeley.edu), and their semantic dependents, which are usually, but not always, their syntactic dependents (including subjects).\\nThe training data was FN annotated sentences.\\nIn testing, participants automatically annotated three previously unseen texts to match gold standard (human) annotation, including predicting previously unseen frames and roles.\\nPrecision and recall were measured both for matching of labels of frames and FEs and for matching of semantic dependency trees based on the annotation.\\nOur shared tasks shows that frame-semantic SRL of running text is a hard problem, partly due to the fact that running text is bound to contain many frames for which no or little annotated training data are available.\\n\",\n          \"Target-dependent Twitter Sentiment Classification\\nSentiment analysis on Twitter data has attracted much attention recently.\\nIn this paper, we focus on target-dependent Twitter sentiment classification; namely, given a query, we classify the sentiments of the tweets as positive, negative or neutral according to whether they contain positive, negative or neutral sentiments about that query.\\nHere the query serves as the target of the sentiments.\\nThe state-of-the-art approaches for solving this problem always adopt the target-independent strategy, which may assign irrelevant sentiments to the given target.\\nMoreover, the state-of-the-art approaches only take the tweet to be classified into consideration when classifying the sentiment; they ignore its context (i.e., related tweets).\\nHowever, because tweets are usually short and more ambiguous, sometimes it is not enough to consider only the current tweet for sentiment classification.\\nIn this paper, we propose to improve target-dependent Twitter sentiment classification by 1) incorporating target-dependent features; and 2) taking related tweets into consideration.\\nAccording to the experimental results, our approach greatly improves the performance of target-dependent sentiment classification.\\nWe incorporate target-dependent features and considers related tweets by utilizing a graph-based optimization.\\nWe combine the target-independent features (content and lexicon) and target-dependent features (rules based on the dependency parsing results) together in subjectivity classification and polarity classification for tweets.\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["split_lengths=[len(dataset[split])for split in dataset]\n","split_lengths"],"metadata":{"id":"lNHA-NChjUJm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711040358400,"user_tz":-330,"elapsed":459,"user":{"displayName":"Rishita Upadhyay","userId":"03411805649589442780"}},"outputId":"1242cf40-dba0-4c7d-b45e-bf6836e0e81a"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[1009, 1009]"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["print(f\"features: {dataset['text'].column_names}\")"],"metadata":{"id":"QuRhC8CLjmGi","colab":{"base_uri":"https://localhost:8080/","height":263},"executionInfo":{"status":"error","timestamp":1711040431975,"user_tz":-330,"elapsed":450,"user":{"displayName":"Rishita Upadhyay","userId":"03411805649589442780"}},"outputId":"982b49b0-b806-499b-d0ea-a275ad7f6d37"},"execution_count":14,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"'Series' object has no attribute 'column_names'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-96d99b24a9ac>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"features: {dataset['text'].column_names}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5900\u001b[0m         ):\n\u001b[1;32m   5901\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5902\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5904\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'column_names'"]}]},{"cell_type":"code","source":[" print(\"\\nDialogue\")\n"," print(dataset_samsum[\"test\"][1][\"dialogue\"])\n"," print(\"\\nSummary\")\n"," print(dataset_samsum[\"test\"][1][\"summary\"])"],"metadata":{"id":"CqLjaM3gj-ko"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pipe=pipeline('summarization',model=model_ckpt)"],"metadata":{"id":"iS3WBpiflSJl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dialogue=dataset_samsum['test'][0]['dialogue']"],"metadata":{"id":"IwGY6YONmYdW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pipeout=pipe(dialogue)\n","pipeout"],"metadata":{"id":"wJlmRPLEmjDl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(pipeout[0]['summary_text'].replace(\" .<n>\", \".\\n\"))"],"metadata":{"id":"dyxT7VPFnCT7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["we will pass the data batch wise. it will look through all the data and give us final accuracy"],"metadata":{"id":"7jZRxox5nlWt"}},{"cell_type":"code","source":["def generate_batch_sized_chunks(list_of_elements, batch_size):\n","    \"\"\"split the dataset into smaller batches that we can process simultaneously\n","    Yield successive batch-sized chunks from list_of_elements.\"\"\"\n","    for i in range(0, len(list_of_elements), batch_size):\n","        yield list_of_elements[i : i + batch_size]"],"metadata":{"id":"BXWt2IVanThR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def calculate_metric_on_test_ds(dataset, metric, model, tokenizer,\n","                               batch_size=16, device=device,\n","                               column_text=\"article\",\n","                               column_summary=\"highlights\"):\n","    article_batches = list(generate_batch_sized_chunks(dataset[column_text], batch_size))\n","    target_batches = list(generate_batch_sized_chunks(dataset[column_summary], batch_size))\n","\n","    for article_batch, target_batch in tqdm(\n","        zip(article_batches, target_batches), total=len(article_batches)):\n","\n","        inputs = tokenizer(article_batch, max_length=1024,  truncation=True,\n","                        padding=\"max_length\", return_tensors=\"pt\")\n","\n","        summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n","                         attention_mask=inputs[\"attention_mask\"].to(device),\n","                         length_penalty=0.8, num_beams=8, max_length=128)\n","        ''' parameter for length penalty ensures that the model does not generate sequences that are too long. '''\n","\n","        # Finally, we decode the generated texts,\n","        # replace the  token, and add the decoded texts with the references to the metric.\n","        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True,\n","                                clean_up_tokenization_spaces=True)\n","               for s in summaries]\n","\n","        decoded_summaries = [d.replace(\"\", \" \") for d in decoded_summaries]\n","\n","\n","        metric.add_batch(predictions=decoded_summaries, references=target_batch)\n","\n","    #  Finally compute and return the ROUGE scores.\n","    score = metric.compute()\n","    return score"],"metadata":{"id":"qZcW6-yfoWvZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rouge_metric=load_metric('rouge')"],"metadata":{"id":"Psrq3-K5pYE8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["score = calculate_metric_on_test_ds(dataset_samsum['test'], rouge_metric, model_pegasus, tokenizer, column_text = 'dialogue', column_summary='summary', batch_size=8)\n"],"metadata":{"id":"LcqKm6BLpEa1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n","rouge_dict = dict((rn, score[rn].mid.fmeasure ) for rn in rouge_names )\n","\n","pd.DataFrame(rouge_dict, index = ['pegasus'])"],"metadata":{"id":"asiNqptqqfj7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def convert_examples_to_features(example_batch):\n","    input_encodings = tokenizer(example_batch['dialogue'] , max_length = 1024, truncation = True )\n","\n","    with tokenizer.as_target_tokenizer():\n","        target_encodings = tokenizer(example_batch['summary'], max_length = 128, truncation = True )\n","\n","    return {\n","        'input_ids' : input_encodings['input_ids'],\n","        'attention_mask': input_encodings['attention_mask'],\n","        'labels': target_encodings['input_ids']\n","    }"],"metadata":{"id":"jRabPTF7sn3o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset_samsum_pt = dataset_samsum.map(convert_examples_to_features, batched = True)\n"],"metadata":{"id":"qCWtVPP-t9t2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset_samsum_pt['train'][0]"],"metadata":{"id":"S4CcqtQouZw5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import DataCollatorForSeq2Seq\n","\n","seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_pegasus)\n"],"metadata":{"id":"dT7jiSYnu4Xe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install accelerate -U\n"],"metadata":{"id":"W5yHQPD51sGG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install transformers[torch]\n","\n","\n"],"metadata":{"id":"NJKnvcPIvcgC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install transformers[integrations]"],"metadata":{"id":"Wnun5ELiTkjh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import TrainingArguments, Trainer, EarlyStoppingCallback"],"metadata":{"id":"a6vmfbpDT3pS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7PoA0EPOT8AP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"AaRIASP2HZOY"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FuXIFTFapAMI"},"outputs":[],"source":["!pip install -q -U bitsandbytes #for reducing floating bits size of training parameters\n","# !pip install -q -U git+https://github.com/huggingface/transformers.git\n","!pip install transformers==4.31\n","!pip install -q -U git+https://github.com/huggingface/peft.git\n","!pip install -q -U git+https://github.com/huggingface/accelerate.git\n","!pip install -q datasets\n","!pip install evaluate\n","!pip install -qqq trl==0.7.1 #helpful library for fine tuning llm for both supervised and rlhf fine tuning"]},{"cell_type":"code","source":["output_dir='pegasus-samsum'"],"metadata":{"id":"D2EyQC2cIYTb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"33r_I44eN7IU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n"],"metadata":{"id":"WO4-soQbM66z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","trainer_args = TrainingArguments(\n","    output_dir='pegasus-samsum', num_train_epochs=1, warmup_steps=500,\n","    per_device_train_batch_size=1, per_device_eval_batch_size=1,\n","    weight_decay=0.01, logging_steps=10,\n","    evaluation_strategy='steps', eval_steps=500, save_steps=1e6,\n","    gradient_accumulation_steps=16\n",")"],"metadata":{"id":"IspDKrqDRFf5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer = Trainer(model=model_pegasus, args=trainer_args,\n","                  tokenizer=tokenizer, data_collator=seq2seq_data_collator,\n","                  train_dataset=dataset_samsum_pt[\"train\"],\n","                  eval_dataset=dataset_samsum_pt[\"validation\"])\n"],"metadata":{"id":"QjPwicJcUXNE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.train()\n"],"metadata":{"id":"49X1gQp_UlO0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_pegasus.save_pretrained(\"pegasus-samsum-model\")"],"metadata":{"id":"NvGY2IZTgMBr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer.save_pretrained(\"tokenizer\")"],"metadata":{"id":"3pinj5f9gRfD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample_text = dataset_samsum[\"test\"][0][\"dialogue\"]\n","\n","reference = dataset_samsum[\"test\"][0][\"summary\"]\n","\n","\n","gen_kwargs = {\"length_penalty\": 0.8, \"num_beams\":8, \"max_length\": 128}\n","\n","pipe = pipeline(\"summarization\", model=\"pegasus-samsum-model\",tokenizer=tokenizer)"],"metadata":{"id":"nJOINdlDgu13"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Dialogue:\")\n","print(sample_text)\n","\n","\n","print(\"\\nReference Summary:\")\n","print(reference)\n","\n","\n","print(\"\\nModel Summary:\")\n","print(pipe(sample_text, **gen_kwargs)[0][\"summary_text\"])"],"metadata":{"id":"dqODO-o-mUPa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample_text = \"\"\"Summarize:A United States plan to build a temporary port off Gaza’s coast to step up the delivery of humanitarian aid has been criticised as an attempt to divert attention from hundreds of thousands of starving Palestinians and Israel’s consistent blocking of assistance to the enclave.\n","\n","US President Joe Biden said in his State of the Union speech on Thursday that he was directing the US military to lead an emergency mission to set up a pier off Gaza’s Mediterranean coast to receive ships carrying food, water, medicine and temporary shelters.\"\"\""],"metadata":{"id":"Fspoo4p2onu1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(pipe(sample_text, **gen_kwargs)[0][\"summary_text\"])"],"metadata":{"id":"evNSANZqpODo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample_text=\"\"\"summarize:Trudy: Hey, so I’m having a party at my place next weekend. Do you want to come?\n","\n","Ruth: Sure! That sounds like fun. Who else is coming?\n","\n","Trudy: Let’s see. I think it’s going to be Jerome, Talia, Anna, Juan, Celeste, Michelle and possibly Jamie. It’s not really going to be a party, more like a small get-together. I’m cooking dinner, and we can just hang out.\n","\n","Ruth: What time should I be there?\n","\n","Trudy: Oh, anytime between 6 and 7 would be fine.\n","\n","Ruth: Can I bring anything?\n","\n","Trudy: Oh, don’t worry about it. I have everything covered.\n","\n","Ruth: Can I at least bring a bottle of wine?\n","\n","Trudy: Well, I’m not going to say no to wine. I’m sure that would be appreciated.\n","\n","Ruth: I’ll do that, then. Thanks for inviting me. \"\"\""],"metadata":{"id":"TE1yZIW8p9rh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"CAg5HGDAqQI4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(pipe(sample_text, **gen_kwargs)[0][\"summary_text\"])"],"metadata":{"id":"UwDUroyaqMfz"},"execution_count":null,"outputs":[]}]}